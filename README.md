# YAML Multi-Agent Orchestration Engine

A declarative AI agent workflow system that enables you to define and execute multi-agent pipelines using simple YAML configuration files.

## ğŸ—ï¸ Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER / CLI                          â”‚
â”‚                                                             â”‚
â”‚   python run.py workflow.yaml --query "User Task"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ORCHESTRATION ENGINE                     â”‚
â”‚                                                             â”‚
â”‚  â€¢ Parses YAML workflow                                     â”‚
â”‚  â€¢ Resolves execution pattern                               â”‚
â”‚    - Sequential / Parallel / Hierarchical                   â”‚
â”‚  â€¢ Mediates ALL memory access & tool calls                  â”‚
â”‚  â€¢ Assembles final prompts for agents (injects context)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                               â”‚
                â”‚                               â”‚
                â”‚                               â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   (Orchestrator retrievesâ”‚     â”‚   (Orchestrator retrievesâ”‚
   â”‚    shared context first) â”‚     â”‚    shared context first) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                               â”‚
                â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      VECTOR DATABASE (ChromaDB)                      â”‚
â”‚                                                                      â”‚
â”‚  â€¢ Stores semantic embeddings, original text, metadata (agent_id,   â”‚
â”‚    workflow_id, timestamp, type)                                     â”‚
â”‚  â€¢ Queried by Orchestrator for Top-K relevant memories               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–²                               â–²
                â”‚                               â”‚
                â”‚  (Orchestrator injects retrieved context into prompts)
                â”‚                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SHORT-TERM WORKFLOW MEMORY   â”‚     â”‚  PROMPT ASSEMBLY      â”‚
â”‚  (in-memory store inside     â”‚     â”‚  (Orchestrator builds â”‚
â”‚   Orchestrator for runtime)  â”‚     â”‚   Role + Goal +       â”‚
â”‚                              â”‚     â”‚   Retrieved Context)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                               â”‚
                â”‚                               â”‚
                â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        AGENT A          â”‚      â”‚        AGENT B          â”‚
â”‚   (Role + Goal + Prompt)â”‚      â”‚   (Role + Goal + Prompt)â”‚
â”‚                         â”‚      â”‚                         â”‚
â”‚  â€¢ Receives final promptâ”‚      â”‚  â€¢ Receives final promptâ”‚
â”‚    from Orchestrator    â”‚      â”‚    from Orchestrator    â”‚
â”‚  â€¢ Executes LLM call    â”‚      â”‚  â€¢ Executes LLM call    â”‚
â”‚  â€¢ When tool needed ->  â”‚      â”‚  â€¢ When tool needed ->  â”‚
â”‚    ASK ORCHESTRATOR     â”‚      â”‚    ASK ORCHESTRATOR     â”‚
â”‚  â€¢ Returns output to    â”‚      â”‚  â€¢ Returns output to    â”‚
â”‚    Orchestrator         â”‚      â”‚    Orchestrator         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                                â”‚
               â”‚                                â”‚
               â”‚                                â”‚
               â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ORCHESTRATOR                                   â”‚
â”‚  â€¢ Receives agent outputs                                               â”‚
â”‚  â€¢ Stores outputs to:                                                   â”‚
â”‚     - SHORT-TERM (workflow context)                                     â”‚
â”‚     - VECTOR DB (long-term memory; creates embeddings & metadata)       â”‚
â”‚  â€¢ Handles agent tool requests:                                         â”‚
â”‚     1) Agent asks Orchestrator for a tool call                          â”‚
â”‚     2) Orchestrator forwards request to MCP Server                      â”‚
â”‚     3) MCP Server processes and returns result to Orchestrator          â”‚
â”‚     4) Orchestrator returns tool result to requesting Agent             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MCP SERVER                           â”‚
â”‚                                                              â”‚
â”‚  â€¢ Exposes tools via Model Context Protocol                  â”‚
â”‚  â€¢ Performs external API calls, DB access, file ops, etc.    â”‚
â”‚  â€¢ Returns results to Orchestrator (never directly to agent) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone and navigate to project
cd final-YAML-Project

# Install dependencies
pip install -r requirements.txt

# Set your API key (choose one)
echo 'GOOGLE_API_KEY=your-gemini-api-key' > .env
# OR
echo 'OPENAI_API_KEY=your-openai-api-key' > .env
```

### 2. Run a Workflow

```bash
# Basic execution
python3 run.py examples/sequential.yaml --query "Tell me about AI"

# Parallel workflow
python3 run.py examples/parallel.yaml --query "Design a todo app"
```

---

## ğŸ“Š Viewing Execution Logs

To see detailed logs of what happens during the generation process, use the `--verbose` flag:

```bash
python3 run.py examples/sequential.yaml --query "Tell me about AI" --verbose
```

### Sample Verbose Output

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Time                â”ƒ Event                     â”ƒ Agent      â”ƒ Details    â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ 2026-01-18T10:13:19 â”‚ vector_memory_initialized â”‚ -          â”‚            â”‚
â”‚ 2026-01-18T10:13:19 â”‚ orchestrator_initialized  â”‚ -          â”‚            â”‚
â”‚ 2026-01-18T10:13:19 â”‚ execution_start           â”‚ -          â”‚            â”‚
â”‚ 2026-01-18T10:13:19 â”‚ sequential_start          â”‚ -          â”‚            â”‚
â”‚ 2026-01-18T10:13:19 â”‚ step_start                â”‚ researcher â”‚            â”‚
â”‚ 2026-01-18T10:13:19 â”‚ agent_start               â”‚ researcher â”‚            â”‚
â”‚ 2026-01-18T10:13:31 â”‚ output_stored             â”‚ researcher â”‚ 9407 chars â”‚
â”‚ 2026-01-18T10:13:31 â”‚ facts_stored              â”‚ researcher â”‚            â”‚
â”‚ 2026-01-18T10:13:31 â”‚ agent_complete            â”‚ researcher â”‚ 9407 chars â”‚
â”‚ 2026-01-18T10:13:31 â”‚ step_complete             â”‚ researcher â”‚ 9407 chars â”‚
â”‚ 2026-01-18T10:13:31 â”‚ step_start                â”‚ writer     â”‚            â”‚
â”‚ 2026-01-18T10:13:31 â”‚ agent_start               â”‚ writer     â”‚            â”‚
â”‚ 2026-01-18T10:13:38 â”‚ output_stored             â”‚ writer     â”‚ 2828 chars â”‚
â”‚ 2026-01-18T10:13:39 â”‚ agent_complete            â”‚ writer     â”‚ 2828 chars â”‚
â”‚ 2026-01-18T10:13:39 â”‚ step_complete             â”‚ writer     â”‚ 2828 chars â”‚
â”‚ 2026-01-18T10:13:39 â”‚ sequential_complete       â”‚ -          â”‚            â”‚
â”‚ 2026-01-18T10:13:39 â”‚ execution_complete        â”‚ -          â”‚            â”‚
â”‚ 2026-01-18T10:13:39 â”‚ memory_saved              â”‚ -          â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Log Events Explained

| Event | Description |
|-------|-------------|
| `vector_memory_initialized` | Vector database for persistent memory is ready |
| `orchestrator_initialized` | Workflow engine is set up |
| `execution_start` | Workflow execution begins |
| `agent_start` | An agent starts processing |
| `output_stored` | Agent output saved to memory |
| `facts_stored` | Key facts extracted and stored |
| `agent_complete` | Agent finished processing |
| `execution_complete` | Entire workflow completed |
| `memory_saved` | All data persisted to disk |

---

## ğŸ“ Log Storage Locations

| Location | Description |
|----------|-------------|
| `memory/` | Workflow execution history and agent outputs |
| `vector_db/` | Semantic vector embeddings for persistent memory |

---

## ğŸ› ï¸ CLI Commands

```bash
# Run workflow
python3 run.py <yaml_file> --query "your query"

# Run with verbose logs
python3 run.py <yaml_file> --query "your query" --verbose

# Validate YAML only (no execution)
python3 run.py --validate <yaml_file>

# Start API server
python3 run.py --server
```

---

## ğŸ”‘ Supported Models

| Provider | Model | Environment Variable |
|----------|-------|---------------------|
| Google | gemini-2.5-flash | `GOOGLE_API_KEY` |
| OpenAI | gpt-4o | `OPENAI_API_KEY` |
| Anthropic | claude-sonnet-4-20250514 | `ANTHROPIC_API_KEY` |

---

## ğŸ“„ Example YAML Structure

```yaml
agents:
  - id: researcher
    role: Research Assistant
    model: gemini
    goal: Find key insights
    instruction: |
      Analyze the topic and provide findings.

  - id: writer
    role: Content Writer
    model: gemini
    goal: Write engaging content
    instruction: |
      Create a summary from the research.

workflow:
  type: sequential
  steps:
    - agent: researcher
    - agent: writer

models:
  gemini:
    provider: google
    model: gemini-2.5-flash
    max_tokens: 4096
    temperature: 0.7
```

---

## ğŸš€ MCP Tools Integration

To use MCP (Model Context Protocol) tools:

### Terminal 1: Start MCP Server
```bash
python3 mcp_server.py
```

### Terminal 2: Run Tool-Enabled Workflow
```bash
python3 run.py examples/submission/03_tool_enabled_data_analysis.yaml --query "Analyze Q4 sales data"
```
